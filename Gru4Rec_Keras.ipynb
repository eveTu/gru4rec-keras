{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gru4Rec - Keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S493qprbsngR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class SessionDataset:\n",
        "\n",
        "      def __init__(self, df):\n",
        "\n",
        "          self.df = df.sort_values(by = ['session', 'timestamp']).reset_index(drop = True) # session (int) | timestamp (int) | item (string)\n",
        "          self.offsets    = np.concatenate((np.zeros(1, dtype = np.int32), self.df.groupby('session').size().cumsum().values))\n",
        "          self.n_sessions = len(self.offsets) - 1\n",
        "\n",
        "          self.item_to_id = {item : i for i, item in enumerate(self.df.item.unique())}\n",
        "          self.id_to_item = {i : item for i, item in self.item_to_id.items()}\n",
        "\n",
        "          self.n_items = len(self.item_to_id)\n",
        "          self.item_to_one_hot = {item : tf.one_hot(self.item_to_id[item], depth = self.n_items) for item in self.item_to_id.keys()}\n",
        "\n",
        "      def extract_session(self, i, one_hot_encoded = True):\n",
        "\n",
        "          session = self.df[self.offsets[i]:self.offsets[i+1]].copy()\n",
        "          if one_hot_encoded:\n",
        "              session.loc[:, 'item'] = session.item.apply(lambda x : self.item_to_one_hot[x])\n",
        "          return session.item.values.tolist()\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL6qBm82U73G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_true = (BATCH_SIZE,)                integer indexes of target items (ground truths)\n",
        "# y_pred = (BATCH_SIZE, 1, n_classes)   next item scores for each item in the batch\n",
        "\n",
        "def BPR(y_true, y_pred):\n",
        "    to_lookup = tf.argmax(y_true, axis = 1)\n",
        "    scores = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_pred), to_lookup))\n",
        "    res = -tf.reduce_sum(tf.math.log(tf.nn.sigmoid(tf.linalg.diag_part(scores) - scores) + 1E-10), axis = 1)\n",
        "    return tf.reduce_sum(res)\n",
        "\n",
        "def TOP1(y_true, y_pred):\n",
        "    to_lookup = tf.argmax(y_true, axis = 1)\n",
        "    scores = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_pred), to_lookup))\n",
        "    #scores = tf.transpose(tf.matmul(y_true, tf.transpose(y_pred)))\n",
        "    diag_scores = tf.linalg.diag_part(scores)\n",
        "    res = tf.reduce_sum(tf.nn.sigmoid(scores - diag_scores) + tf.nn.sigmoid(tf.square(scores)) - tf.nn.sigmoid(tf.square(diag_scores)), axis = 1)\n",
        "    return tf.reduce_sum(res)"
      ],
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUa04ZQr2FF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TOP1(y_true, y_pred):\n",
        "    _y_pred = tf.expand_dims(y_pred, axis = -1)  # (BATCH_SIZE, n_classes) ---> (BATCH_SIZE, n_classes, 1) \n",
        "    mat = tf.matmul(tf.expand_dims(tf.ones_like(y_true), -1), tf.expand_dims(y_true, axis = 1)) # (BATCH_SIZE, n_classes, 1) x (BATCH_SIZE, 1, n_classes) --> (BATCH_SIZE, n_classes, n_classes)\n",
        "    score_diffs = tf.matmul(mat, _y_pred) # (BATCH_SIZE, n_classes, n_classes) x (BATCH_SIZE, n_classes, 1) --> (BATCH_SIZE, n_classes, 1)\n",
        "    score_diffs = tf.squeeze(score_diffs - _y_pred, -1) # (BATCH_SIZE, n_classes)\n",
        "    loss_by_sample = tf.reduce_sum(tf.nn.sigmoid(tf.square(y_pred)), axis = -1) + \\\n",
        "                      tf.reduce_sum(tf.sigmoid(-score_diffs), axis = -1) + \\\n",
        "                    -tf.squeeze(tf.squeeze(tf.nn.sigmoid(tf.square(tf.matmul(tf.expand_dims(y_true, 1), _y_pred))), -1), -1)\n",
        "    return tf.reduce_sum(loss_by_sample)\n",
        "\n",
        "def BPR(y_true, y_pred):  # both inputs have shape (BATCH_SIZE, n_classes)\n",
        "    _y_pred = tf.expand_dims(y_pred, axis = -1)  # (BATCH_SIZE, n_classes, 1) \n",
        "    mat = tf.matmul(tf.expand_dims(tf.ones_like(y_true), -1), tf.expand_dims(y_true, axis = 1)) # (BATCH_SIZE, n_classes, 1) x (BATCH_SIZE, 1, n_classes) = (BATCH_SIZE, n_classes, n_classes)\n",
        "    score_diffs = tf.matmul(mat, _y_pred) # (BATCH_SIZE, n_classes, n_classes) x (BATCH_SIZE, n_classes, 1) = (BATCH_SIZE, n_classes, 1)\n",
        "    score_diffs = tf.squeeze(score_diffs - _y_pred, -1) # (BATCH_SIZE, n_classes)\n",
        "    return -tf.reduce_sum(tf.math.log(tf.nn.sigmoid(score_diffs)))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYWjZhJHXcHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Gru4Rec:\n",
        "\n",
        "    def __init__(self, n_classes, n_layers = 1, n_hidden = 64, loss = TOP1, batch_size = 16):\n",
        "\n",
        "        self.n_classes  = n_classes   # = number of items\n",
        "\n",
        "        self.n_layers = n_layers  # number of stacked GRU layers\n",
        "        self.n_hidden = n_hidden  # dimension of GRU cell's hidden state\n",
        "        self.loss     = loss\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        model = tf.keras.models.Sequential()\n",
        "        for i in range(self.n_layers):\n",
        "            model.add(tf.keras.layers.GRU(name = 'GRU_{}'.format(i+1),\n",
        "                                          units      = self.n_hidden, \n",
        "                                          activation = 'relu', \n",
        "                                          stateful   = True,\n",
        "                                          return_sequences = (i < self.n_layers - 1)))\n",
        "        model.add(tf.keras.layers.Dense(units = self.n_classes, activation = 'linear'))\n",
        "\n",
        "        top3accuracy = lambda y_true, y_pred: tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k = 3)\n",
        "        top3accuracy.__name__ = 'top3accuracy'\n",
        "        model.compile(loss = self.loss, optimizer = 'adam', metrics = ['accuracy', top3accuracy])\n",
        "\n",
        "        model.build(input_shape = (self.batch_size, 1, self.n_classes))\n",
        "        print(model.summary())\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _reset_hidden(self, i):\n",
        "\n",
        "        for nl, layer in enumerate(self.model.layers):   # session has change: reset related hidden state\n",
        "            if layer.name.startswith('GRU_') and layer.states[0] is not None:\n",
        "                hidden_updated = layer.states[0].numpy()\n",
        "                hidden_updated[i, :] = 0.\n",
        "                self.model.layers[nl].reset_states(hidden_updated)\n",
        "\n",
        "    def train_batch_generator(self, dataset):  # session | item | timestamp\n",
        "\n",
        "      assert dataset.n_sessions > self.batch_size, \"Training set is too small\"\n",
        "      ixs = np.arange(dataset.n_sessions)\n",
        "\n",
        "      stacks = [[]] * self.batch_size\n",
        "      next_session_id = 0\n",
        "\n",
        "      X, y = np.empty(shape = (self.batch_size, 1, self.n_classes)), np.empty(shape = (self.batch_size, self.n_classes))\n",
        "      X[:], y[:] = None, None\n",
        "      while True:\n",
        "          for i in range(self.batch_size):\n",
        "              # 1. If stack i has only one element: change session\n",
        "              if len(stacks[i]) <= 1:\n",
        "                  if next_session_id >= dataset.n_sessions: # no more sessions available: shuffle sessions and restart\n",
        "                      np.random.shuffle(ixs)\n",
        "                      next_session_id = 0\n",
        "                  while not len(stacks[i]) >= 2:   # ignore sessions with only one element\n",
        "                      stacks[i] = dataset.extract_session(ixs[next_session_id])[::-1]\n",
        "                      next_session_id += 1\n",
        "                  self._reset_hidden(i)\n",
        "              # 2. Stack i is now valid: set input + target variables\n",
        "              X[i, 0] = stacks[i].pop()\n",
        "              y[i]    = stacks[i][-1]\n",
        "\n",
        "          yield tf.constant(X, dtype = tf.float32), tf.constant(y, dtype = tf.float32)\n",
        "\n",
        "    def fit(self, dataset, steps_per_epoch = 10000, epochs = 5):\n",
        "\n",
        "        checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = \"gru-chkpt-{epoch:02d}.hdf5\")\n",
        "        self.model.fit_generator(generator       = self.train_batch_generator(dataset), \n",
        "                                 steps_per_epoch = steps_per_epoch, \n",
        "                                 epochs          = epochs, \n",
        "                                 callbacks       = [checkpoint], \n",
        "                                 shuffle         = False)\n",
        "\n",
        "    def get_final_hidden_states(self, dataset):\n",
        "\n",
        "        final_states = np.empty(shape = (dataset.n_sessions, self.n_layers, self.n_hidden))\n",
        "        final_states[:] = None\n",
        "        done = [False] * dataset.n_sessions\n",
        "\n",
        "        stacks = [dataset.extract_session(i)[::-1] for i in range(self.batch_size)]   # events are in reverse time order\n",
        "        next_session_id = self.batch_size\n",
        "        batch_idx_to_session = np.arange(self.batch_size)\n",
        "        X = np.empty(shape = (self.batch_size, 1, self.n_classes))\n",
        "\n",
        "        self.model.reset_states()\n",
        "\n",
        "        n_done = 0\n",
        "        while n_done < dataset.n_sessions:\n",
        "            for i in range(self.batch_size):\n",
        "                while len(stacks[i]) == 1:\n",
        "                    if not done[batch_idx_to_session[i]]:\n",
        "                        final_states[batch_idx_to_session[i], :] = np.array([layer.states[0][i, :] for layer in self.model.layers if layer.name.startswith('GRU_')])\n",
        "                        done[batch_idx_to_session[i]] = True\n",
        "                        n_done += 1\n",
        "                        if n_done % 100 == 0:\n",
        "                            print(\"{} / {}\".format(n_done, dataset.n_sessions))\n",
        "                    if next_session_id >= dataset.n_sessions: # restart from the beginning\n",
        "                        next_session_id = 0\n",
        "                    stacks[i] = dataset.extract_session(next_session_id)[::-1]\n",
        "                    batch_idx_to_session[i] = next_session_id\n",
        "                    next_session_id += 1\n",
        "                    self._reset_hidden(i)   # session has changes --> reset corresponding hidden state\n",
        "                X[i, 0] = stacks[i].pop()\n",
        "\n",
        "            _ = self.model.predict(X)   # hidden states get updated\n",
        "            \n",
        "        return final_states\n",
        "        "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXH0nVjSGEed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"workouts_clean_2.csv\").sort_values(by = ['session', 'timestamp']).reset_index(drop = True)\n",
        "offsets = np.concatenate((np.zeros(1, dtype = np.int32), df.groupby('session').size().cumsum().values))\n",
        "\n",
        "dataset_train = SessionDataset(df.iloc[~df.index.isin(offsets[1:] - 1)])    # training set: remove last element from each session\n",
        "\n",
        "X_test = df.iloc[offsets[1:] - 2][['session', 'item']].sort_values(by = ['session']).reset_index(drop = True)\n",
        "y_test = df.iloc[offsets[1:] - 1][['session', 'item']].sort_values(by = ['session']).reset_index(drop = True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMj1va3RaHc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "0a5a8306-d641-4ee3-ff3e-e37f90a8b889"
      },
      "source": [
        "g4r = Gru4Rec(n_classes = dataset_train.n_items)\n",
        "g4r.fit(dataset_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer GRU_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "GRU_1 (GRU)                  multiple                  21888     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  3120      \n",
            "=================================================================\n",
            "Total params: 25,008\n",
            "Trainable params: 25,008\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "10000/10000 [==============================] - 45s 5ms/step - loss: 420.9508 - accuracy: 0.7637 - top3accuracy: 0.9607\n",
            "Epoch 2/5\n",
            "10000/10000 [==============================] - 46s 5ms/step - loss: 408.2956 - accuracy: 0.7986 - top3accuracy: 0.9770\n",
            "Epoch 3/5\n",
            "10000/10000 [==============================] - 45s 5ms/step - loss: 406.8640 - accuracy: 0.8008 - top3accuracy: 0.9812\n",
            "Epoch 4/5\n",
            "10000/10000 [==============================] - 45s 4ms/step - loss: 404.8083 - accuracy: 0.8148 - top3accuracy: 0.9818\n",
            "Epoch 5/5\n",
            "10000/10000 [==============================] - 45s 5ms/step - loss: 404.3906 - accuracy: 0.8115 - top3accuracy: 0.9826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYwwY25mosmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "e1e51fe9-70a1-4e1a-e71d-bd4aaa87a196"
      },
      "source": [
        "final_states = g4r.get_final_hidden_states(dataset_train)\n",
        "np.save('final_states.npy', final_states, allow_pickle = False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 / 1082\n",
            "200 / 1082\n",
            "300 / 1082\n",
            "400 / 1082\n",
            "500 / 1082\n",
            "600 / 1082\n",
            "700 / 1082\n",
            "800 / 1082\n",
            "900 / 1082\n",
            "1000 / 1082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOgKazJsDUJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "final_states = np.load('final_states.npy')\n",
        "\n",
        "g4r.model.reset_states()\n",
        "\n",
        "rem = dataset_train.n_sessions % g4r.batch_size\n",
        "if rem > 0:\n",
        "  X_test = pd.concat((X_test, X_test[:(g4r.batch_size - rem)]), axis = 0)\n",
        "\n",
        "y_pred = np.empty(shape = (dataset_train.n_sessions, g4r.n_classes))\n",
        "y_pred[:] = None\n",
        "X = np.empty(shape = (g4r.batch_size, 1, g4r.n_classes))\n",
        "for batch_id in range(dataset_train.n_sessions // g4r.batch_size):\n",
        "    X[:] = None\n",
        "    for i in range(g4r.batch_size):\n",
        "        X[i, :] = dataset_train.item_to_one_hot[X_test.iloc[batch_id * g4r.batch_size + i]['item']]\n",
        "    nlg = 0\n",
        "    for nl, layer in enumerate(g4r.model.layers):\n",
        "        if layer.name.startswith('GRU_'):\n",
        "            g4r.model.layers[nl].reset_states(final_states[batch_id * g4r.batch_size : (batch_id + 1) * g4r.batch_size, nlg, :])\n",
        "            nlg += 1\n",
        "    y_pred[batch_id * g4r.batch_size : (batch_id + 1) * g4r.batch_size, :] = g4r.model.predict(X)[:g4r.batch_size]\n",
        "\n",
        "y_pred = tf.constant(y_pred[:dataset_train.n_sessions], dtype = tf.float32)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwEr9AQ8DezI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = np.empty(shape = (dataset_train.n_sessions, dataset_train.n_items))\n",
        "for i in range(y_true.shape[0]):\n",
        "    y_true[i, :] = dataset_train.item_to_one_hot[y_test.item.values[i]]\n",
        "y_true = tf.constant(y_true, dtype = tf.float32)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWzJCzaqGe7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c6553c8-1ab6-40ee-f6c4-75a5b20878a0"
      },
      "source": [
        "tf.reduce_sum(tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k = 3)) / y_true.shape[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.9537893>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}