{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gru4Rec - Keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S493qprbsngR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class SessionDataset:\n",
        "\n",
        "      def __init__(self, df):\n",
        "\n",
        "          self.df = df.sort_values(by = ['session', 'timestamp']).reset_index(drop = True) # session (int) | timestamp (int) | item (string)\n",
        "          self.offsets    = np.concatenate((np.zeros(1, dtype = np.int32), self.df.groupby('session').size().cumsum().values))\n",
        "          self.n_sessions = len(self.offsets) - 1\n",
        "\n",
        "          self.item_to_id = {item : i for i, item in enumerate(self.df.item.unique())}\n",
        "          self.id_to_item = {i : item for i, item in self.item_to_id.items()}\n",
        "\n",
        "          self.n_items = len(self.item_to_id)\n",
        "          self.item_to_one_hot = {item : tf.one_hot(self.item_to_id[item], depth = self.n_items) for item in self.item_to_id.keys()}\n",
        "\n",
        "      def extract_session(self, i, one_hot_encoded = True):\n",
        "\n",
        "          session = self.df[self.offsets[i]:self.offsets[i+1]].copy()\n",
        "          if one_hot_encoded:\n",
        "              session.loc[:, 'item'] = session.item.apply(lambda x : self.item_to_one_hot[x])\n",
        "          return session.item.values.tolist()\n",
        "\n",
        "df = pd.read_csv(\"workouts_clean_2.csv\").sort_values(by = ['session', 'timestamp']).reset_index(drop = True)\n",
        "sizes = df.groupby('session').size()\n",
        "sizes = sizes[sizes > 2].index\n",
        "df = df.query(\"session in @sizes\").sort_values(by = ['session', 'timestamp']).reset_index(drop = True)\n",
        "\n",
        "offsets = np.concatenate((np.zeros(1, dtype = np.int32), df.groupby('session').size().cumsum().values))\n",
        "\n",
        "dfTest  = df.iloc[offsets[1:] - 1]\n",
        "dfTrain = df.iloc[list(set(df.index) - set(dfTest.index))]\n",
        "\n",
        "dataset = SessionDataset(dfTrain)"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RGM_l3kXQMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOSSES\n",
        "\n",
        "def TOP1(y_true, y_pred):\n",
        "    _y_pred = tf.expand_dims(y_pred, axis = -1)  # (BATCH_SIZE, n_classes) ---> (BATCH_SIZE, n_classes, 1) \n",
        "    mat = tf.matmul(tf.expand_dims(tf.ones_like(y_true), -1), tf.expand_dims(y_true, axis = 1)) # (BATCH_SIZE, n_classes, 1) x (BATCH_SIZE, 1, n_classes) --> (BATCH_SIZE, n_classes, n_classes)\n",
        "    score_diffs = tf.matmul(mat, _y_pred) # (BATCH_SIZE, n_classes, n_classes) x (BATCH_SIZE, n_classes, 1) --> (BATCH_SIZE, n_classes, 1)\n",
        "    score_diffs = tf.squeeze(score_diffs - _y_pred, -1) # (BATCH_SIZE, n_classes)\n",
        "    loss_by_sample = tf.reduce_sum(tf.nn.sigmoid(tf.square(y_pred)), axis = -1) + \\\n",
        "                      tf.reduce_sum(tf.sigmoid(-score_diffs), axis = -1) + \\\n",
        "                    -tf.squeeze(tf.squeeze(tf.nn.sigmoid(tf.square(tf.matmul(tf.expand_dims(y_true, 1), _y_pred))), -1), -1)\n",
        "    return tf.reduce_sum(loss_by_sample)\n",
        "\n",
        "def BPR(y_true, y_pred):  # both inputs have shape (BATCH_SIZE, n_classes)\n",
        "    _y_pred = tf.expand_dims(y_pred, axis = -1)  # (BATCH_SIZE, n_classes, 1) \n",
        "    mat = tf.matmul(tf.expand_dims(tf.ones_like(y_true), -1), tf.expand_dims(y_true, axis = 1)) # (BATCH_SIZE, n_classes, 1) x (BATCH_SIZE, 1, n_classes) = (BATCH_SIZE, n_classes, n_classes)\n",
        "    score_diffs = tf.matmul(mat, _y_pred) # (BATCH_SIZE, n_classes, n_classes) x (BATCH_SIZE, n_classes, 1) = (BATCH_SIZE, n_classes, 1)\n",
        "    score_diffs = tf.squeeze(score_diffs - _y_pred, -1) # (BATCH_SIZE, n_classes)\n",
        "    return -tf.reduce_sum(tf.math.log(tf.nn.sigmoid(score_diffs)))\n"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYWjZhJHXcHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Gru4Rec:\n",
        "\n",
        "    def __init__(self, n_classes, n_layers = 1, n_hidden = 128, loss = 'TOP1', batch_size = 32):\n",
        "\n",
        "        self.n_classes  = n_classes   # = number of items\n",
        "\n",
        "        self.n_layers = n_layers  # number of stacked GRU layers\n",
        "        self.n_hidden = n_hidden  # dimension of GRU cell's hidden state\n",
        "\n",
        "        self.model = self.build_model()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        model = tf.keras.models.Sequential()\n",
        "        for i in range(self.n_layers):\n",
        "            model.add(tf.keras.layers.GRU(name = 'GRU_{}'.format(i+1),\n",
        "                                          units      = self.n_hidden, \n",
        "                                          activation = 'relu', \n",
        "                                          stateful   = True,\n",
        "                                          return_sequences = (i < self.n_layers - 1)))\n",
        "        model.add(tf.keras.layers.Dense(units = self.n_classes, activation = 'linear'))\n",
        "\n",
        "        top3accuracy = lambda y_true, y_pred: tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k = 3)\n",
        "        top3accuracy.__name__ = 'top3accuracy'\n",
        "        model.compile(loss = TOP1, optimizer = 'adam', metrics = ['accuracy', top3accuracy])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _reset_hidden(self, i):\n",
        "\n",
        "        for nl in range(len(self.model.layers)):   # session has change: reset related hidden state\n",
        "            if self.model.layers[nl].name.startswith('GRU_'):\n",
        "                hidden_updated = self.model.layers[nl].states[0].numpy()\n",
        "                hidden_updated[i, :] = 0.\n",
        "                self.model.layers[nl].reset_states(hidden_updated)\n",
        "\n",
        "    def train_batch_generator(self, dataset):  # session | item | timestamp\n",
        "\n",
        "      assert dataset.n_sessions > self.batch_size, \"Training set is too small\"\n",
        "\n",
        "      ixs = np.arange(dataset.n_sessions)\n",
        "      stacks = [dataset.extract_session(ixs[i])[::-1] for i in range(self.batch_size)]   # events are in reverse time order\n",
        "      next_session_id = self.batch_size\n",
        "\n",
        "      X, y = np.empty(shape = (self.batch_size, 1, self.n_classes)), np.empty(shape = (self.batch_size, self.n_classes))\n",
        "      while True:\n",
        "          for i in range(self.batch_size):\n",
        "              # 1. If stack i has only one element: change session\n",
        "              if len(stacks[i]) <= 1:\n",
        "                  if next_session_id >= dataset.n_sessions: # no more sessions available: shuffle sessions and restart\n",
        "                      np.random.shuffle(ixs)\n",
        "                      next_session_id = 0\n",
        "                  stacks[i] = dataset.extract_session(ixs[next_session_id])[::-1]\n",
        "                  next_session_id += 1\n",
        "                  self._reset_hidden(i)\n",
        "              # 2. Stack i is now valid: set input + target variables\n",
        "              X[i, 0] = stacks[i].pop()\n",
        "              y[i]    = stacks[i][-1]\n",
        "\n",
        "          yield tf.constant(X, dtype = tf.float32), tf.constant(y, dtype = tf.float32)\n",
        "\n",
        "    def fit(self, dataset, steps_per_epoch = 10000, epochs = 10):\n",
        "\n",
        "        checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = \"gru-chkpt-{epoch:02d}.hdf5\")\n",
        "        self.model.fit_generator(generator       = self.train_batch_generator(dataset), \n",
        "                                 steps_per_epoch = steps_per_epoch, \n",
        "                                 epochs          = epochs, \n",
        "                                 callbacks       = [checkpoint], \n",
        "                                 shuffle         = False)\n",
        "\n",
        "    def get_final_hidden_states(self, dataset):\n",
        "\n",
        "        final_states = np.empty(shape = (dataset.n_sessions, self.n_layers, self.n_hidden))\n",
        "        final_states[:] = None\n",
        "        done = [False] * dataset.n_sessions\n",
        "\n",
        "        stacks = [dataset.extract_session(i)[::-1] for i in range(self.batch_size)]   # events are in reverse time order\n",
        "        next_session_id = self.batch_size\n",
        "        batch_idx_to_session = np.arange(self.batch_size)\n",
        "        X = np.empty(shape = (self.batch_size, 1, self.n_classes))\n",
        "\n",
        "        self.model.reset_states()\n",
        "\n",
        "        n_done = 0\n",
        "        while n_done < dataset.n_sessions:\n",
        "            for i in range(self.batch_size):\n",
        "                if len(stacks[i]) <= 1:\n",
        "                    if not done[batch_idx_to_session[i]]:\n",
        "                        final_states[batch_idx_to_session[i], :] = np.array([layer.states[0][i, :] for layer in self.model.layers if layer.name.startswith('GRU_')])\n",
        "                        done[batch_idx_to_session[i]] = True\n",
        "                        n_done += 1\n",
        "                        print(\"{} / {}\".format(n_done, dataset.n_sessions))\n",
        "                    if next_session_id >= dataset.n_sessions: # restart from the beginning\n",
        "                        next_session_id = 0\n",
        "                    stacks[i] = dataset.extract_session(next_session_id)[::-1]\n",
        "                    batch_idx_to_session[i] = next_session_id\n",
        "                    next_session_id += 1\n",
        "                    self._reset_hidden(i)   # session has changes --> reset corresponding hidden state\n",
        "                X[i, 0] = stacks[i].pop()\n",
        "\n",
        "            _ = self.model.predict(X)   # hidden states get updated\n",
        "            \n",
        "        return final_states\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMj1va3RaHc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "b0872d24-7722-4cc2-cd9d-cc0b23e3d41f"
      },
      "source": [
        "g4r = Gru4Rec(n_classes = dataset.n_items)\n",
        "g4r.fit(dataset)"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 41s 4ms/step - loss: 832.2532 - accuracy: 0.7291 - top3accuracy: 0.9677\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 43s 4ms/step - loss: 812.6177 - accuracy: 0.8078 - top3accuracy: 0.9810\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 44s 4ms/step - loss: 808.2551 - accuracy: 0.8112 - top3accuracy: 0.9826\n",
            "Epoch 4/10\n",
            " 1349/10000 [===>..........................] - ETA: 35s - loss: 806.9630 - accuracy: 0.8047 - top3accuracy: 0.9848"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-258-351258ee68e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mg4r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGru4Rec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mg4r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-257-e9fd927c4ac1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, steps_per_epoch, epochs)\u001b[0m\n\u001b[1;32m     69\u001b[0m                                  \u001b[0mepochs\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                  \u001b[0mcallbacks\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                                  shuffle         = False)\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_final_hidden_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-tQxNGik2rd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c0ad2460-8568-4ec2-897b-5f92954006d0"
      },
      "source": [
        "final_states = g4r.get_final_hidden_states(dataset)"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 1059\n",
            "2 / 1059\n",
            "3 / 1059\n",
            "4 / 1059\n",
            "5 / 1059\n",
            "6 / 1059\n",
            "7 / 1059\n",
            "8 / 1059\n",
            "9 / 1059\n",
            "10 / 1059\n",
            "11 / 1059\n",
            "12 / 1059\n",
            "13 / 1059\n",
            "14 / 1059\n",
            "15 / 1059\n",
            "16 / 1059\n",
            "17 / 1059\n",
            "18 / 1059\n",
            "19 / 1059\n",
            "20 / 1059\n",
            "21 / 1059\n",
            "22 / 1059\n",
            "23 / 1059\n",
            "24 / 1059\n",
            "25 / 1059\n",
            "26 / 1059\n",
            "27 / 1059\n",
            "28 / 1059\n",
            "29 / 1059\n",
            "30 / 1059\n",
            "31 / 1059\n",
            "32 / 1059\n",
            "33 / 1059\n",
            "34 / 1059\n",
            "35 / 1059\n",
            "36 / 1059\n",
            "37 / 1059\n",
            "38 / 1059\n",
            "39 / 1059\n",
            "40 / 1059\n",
            "41 / 1059\n",
            "42 / 1059\n",
            "43 / 1059\n",
            "44 / 1059\n",
            "45 / 1059\n",
            "46 / 1059\n",
            "47 / 1059\n",
            "48 / 1059\n",
            "49 / 1059\n",
            "50 / 1059\n",
            "51 / 1059\n",
            "52 / 1059\n",
            "53 / 1059\n",
            "54 / 1059\n",
            "55 / 1059\n",
            "56 / 1059\n",
            "57 / 1059\n",
            "58 / 1059\n",
            "59 / 1059\n",
            "60 / 1059\n",
            "61 / 1059\n",
            "62 / 1059\n",
            "63 / 1059\n",
            "64 / 1059\n",
            "65 / 1059\n",
            "66 / 1059\n",
            "67 / 1059\n",
            "68 / 1059\n",
            "69 / 1059\n",
            "70 / 1059\n",
            "71 / 1059\n",
            "72 / 1059\n",
            "73 / 1059\n",
            "74 / 1059\n",
            "75 / 1059\n",
            "76 / 1059\n",
            "77 / 1059\n",
            "78 / 1059\n",
            "79 / 1059\n",
            "80 / 1059\n",
            "81 / 1059\n",
            "82 / 1059\n",
            "83 / 1059\n",
            "84 / 1059\n",
            "85 / 1059\n",
            "86 / 1059\n",
            "87 / 1059\n",
            "88 / 1059\n",
            "89 / 1059\n",
            "90 / 1059\n",
            "91 / 1059\n",
            "92 / 1059\n",
            "93 / 1059\n",
            "94 / 1059\n",
            "95 / 1059\n",
            "96 / 1059\n",
            "97 / 1059\n",
            "98 / 1059\n",
            "99 / 1059\n",
            "100 / 1059\n",
            "101 / 1059\n",
            "102 / 1059\n",
            "103 / 1059\n",
            "104 / 1059\n",
            "105 / 1059\n",
            "106 / 1059\n",
            "107 / 1059\n",
            "108 / 1059\n",
            "109 / 1059\n",
            "110 / 1059\n",
            "111 / 1059\n",
            "112 / 1059\n",
            "113 / 1059\n",
            "114 / 1059\n",
            "115 / 1059\n",
            "116 / 1059\n",
            "117 / 1059\n",
            "118 / 1059\n",
            "119 / 1059\n",
            "120 / 1059\n",
            "121 / 1059\n",
            "122 / 1059\n",
            "123 / 1059\n",
            "124 / 1059\n",
            "125 / 1059\n",
            "126 / 1059\n",
            "127 / 1059\n",
            "128 / 1059\n",
            "129 / 1059\n",
            "130 / 1059\n",
            "131 / 1059\n",
            "132 / 1059\n",
            "133 / 1059\n",
            "134 / 1059\n",
            "135 / 1059\n",
            "136 / 1059\n",
            "137 / 1059\n",
            "138 / 1059\n",
            "139 / 1059\n",
            "140 / 1059\n",
            "141 / 1059\n",
            "142 / 1059\n",
            "143 / 1059\n",
            "144 / 1059\n",
            "145 / 1059\n",
            "146 / 1059\n",
            "147 / 1059\n",
            "148 / 1059\n",
            "149 / 1059\n",
            "150 / 1059\n",
            "151 / 1059\n",
            "152 / 1059\n",
            "153 / 1059\n",
            "154 / 1059\n",
            "155 / 1059\n",
            "156 / 1059\n",
            "157 / 1059\n",
            "158 / 1059\n",
            "159 / 1059\n",
            "160 / 1059\n",
            "161 / 1059\n",
            "162 / 1059\n",
            "163 / 1059\n",
            "164 / 1059\n",
            "165 / 1059\n",
            "166 / 1059\n",
            "167 / 1059\n",
            "168 / 1059\n",
            "169 / 1059\n",
            "170 / 1059\n",
            "171 / 1059\n",
            "172 / 1059\n",
            "173 / 1059\n",
            "174 / 1059\n",
            "175 / 1059\n",
            "176 / 1059\n",
            "177 / 1059\n",
            "178 / 1059\n",
            "179 / 1059\n",
            "180 / 1059\n",
            "181 / 1059\n",
            "182 / 1059\n",
            "183 / 1059\n",
            "184 / 1059\n",
            "185 / 1059\n",
            "186 / 1059\n",
            "187 / 1059\n",
            "188 / 1059\n",
            "189 / 1059\n",
            "190 / 1059\n",
            "191 / 1059\n",
            "192 / 1059\n",
            "193 / 1059\n",
            "194 / 1059\n",
            "195 / 1059\n",
            "196 / 1059\n",
            "197 / 1059\n",
            "198 / 1059\n",
            "199 / 1059\n",
            "200 / 1059\n",
            "201 / 1059\n",
            "202 / 1059\n",
            "203 / 1059\n",
            "204 / 1059\n",
            "205 / 1059\n",
            "206 / 1059\n",
            "207 / 1059\n",
            "208 / 1059\n",
            "209 / 1059\n",
            "210 / 1059\n",
            "211 / 1059\n",
            "212 / 1059\n",
            "213 / 1059\n",
            "214 / 1059\n",
            "215 / 1059\n",
            "216 / 1059\n",
            "217 / 1059\n",
            "218 / 1059\n",
            "219 / 1059\n",
            "220 / 1059\n",
            "221 / 1059\n",
            "222 / 1059\n",
            "223 / 1059\n",
            "224 / 1059\n",
            "225 / 1059\n",
            "226 / 1059\n",
            "227 / 1059\n",
            "228 / 1059\n",
            "229 / 1059\n",
            "230 / 1059\n",
            "231 / 1059\n",
            "232 / 1059\n",
            "233 / 1059\n",
            "234 / 1059\n",
            "235 / 1059\n",
            "236 / 1059\n",
            "237 / 1059\n",
            "238 / 1059\n",
            "239 / 1059\n",
            "240 / 1059\n",
            "241 / 1059\n",
            "242 / 1059\n",
            "243 / 1059\n",
            "244 / 1059\n",
            "245 / 1059\n",
            "246 / 1059\n",
            "247 / 1059\n",
            "248 / 1059\n",
            "249 / 1059\n",
            "250 / 1059\n",
            "251 / 1059\n",
            "252 / 1059\n",
            "253 / 1059\n",
            "254 / 1059\n",
            "255 / 1059\n",
            "256 / 1059\n",
            "257 / 1059\n",
            "258 / 1059\n",
            "259 / 1059\n",
            "260 / 1059\n",
            "261 / 1059\n",
            "262 / 1059\n",
            "263 / 1059\n",
            "264 / 1059\n",
            "265 / 1059\n",
            "266 / 1059\n",
            "267 / 1059\n",
            "268 / 1059\n",
            "269 / 1059\n",
            "270 / 1059\n",
            "271 / 1059\n",
            "272 / 1059\n",
            "273 / 1059\n",
            "274 / 1059\n",
            "275 / 1059\n",
            "276 / 1059\n",
            "277 / 1059\n",
            "278 / 1059\n",
            "279 / 1059\n",
            "280 / 1059\n",
            "281 / 1059\n",
            "282 / 1059\n",
            "283 / 1059\n",
            "284 / 1059\n",
            "285 / 1059\n",
            "286 / 1059\n",
            "287 / 1059\n",
            "288 / 1059\n",
            "289 / 1059\n",
            "290 / 1059\n",
            "291 / 1059\n",
            "292 / 1059\n",
            "293 / 1059\n",
            "294 / 1059\n",
            "295 / 1059\n",
            "296 / 1059\n",
            "297 / 1059\n",
            "298 / 1059\n",
            "299 / 1059\n",
            "300 / 1059\n",
            "301 / 1059\n",
            "302 / 1059\n",
            "303 / 1059\n",
            "304 / 1059\n",
            "305 / 1059\n",
            "306 / 1059\n",
            "307 / 1059\n",
            "308 / 1059\n",
            "309 / 1059\n",
            "310 / 1059\n",
            "311 / 1059\n",
            "312 / 1059\n",
            "313 / 1059\n",
            "314 / 1059\n",
            "315 / 1059\n",
            "316 / 1059\n",
            "317 / 1059\n",
            "318 / 1059\n",
            "319 / 1059\n",
            "320 / 1059\n",
            "321 / 1059\n",
            "322 / 1059\n",
            "323 / 1059\n",
            "324 / 1059\n",
            "325 / 1059\n",
            "326 / 1059\n",
            "327 / 1059\n",
            "328 / 1059\n",
            "329 / 1059\n",
            "330 / 1059\n",
            "331 / 1059\n",
            "332 / 1059\n",
            "333 / 1059\n",
            "334 / 1059\n",
            "335 / 1059\n",
            "336 / 1059\n",
            "337 / 1059\n",
            "338 / 1059\n",
            "339 / 1059\n",
            "340 / 1059\n",
            "341 / 1059\n",
            "342 / 1059\n",
            "343 / 1059\n",
            "344 / 1059\n",
            "345 / 1059\n",
            "346 / 1059\n",
            "347 / 1059\n",
            "348 / 1059\n",
            "349 / 1059\n",
            "350 / 1059\n",
            "351 / 1059\n",
            "352 / 1059\n",
            "353 / 1059\n",
            "354 / 1059\n",
            "355 / 1059\n",
            "356 / 1059\n",
            "357 / 1059\n",
            "358 / 1059\n",
            "359 / 1059\n",
            "360 / 1059\n",
            "361 / 1059\n",
            "362 / 1059\n",
            "363 / 1059\n",
            "364 / 1059\n",
            "365 / 1059\n",
            "366 / 1059\n",
            "367 / 1059\n",
            "368 / 1059\n",
            "369 / 1059\n",
            "370 / 1059\n",
            "371 / 1059\n",
            "372 / 1059\n",
            "373 / 1059\n",
            "374 / 1059\n",
            "375 / 1059\n",
            "376 / 1059\n",
            "377 / 1059\n",
            "378 / 1059\n",
            "379 / 1059\n",
            "380 / 1059\n",
            "381 / 1059\n",
            "382 / 1059\n",
            "383 / 1059\n",
            "384 / 1059\n",
            "385 / 1059\n",
            "386 / 1059\n",
            "387 / 1059\n",
            "388 / 1059\n",
            "389 / 1059\n",
            "390 / 1059\n",
            "391 / 1059\n",
            "392 / 1059\n",
            "393 / 1059\n",
            "394 / 1059\n",
            "395 / 1059\n",
            "396 / 1059\n",
            "397 / 1059\n",
            "398 / 1059\n",
            "399 / 1059\n",
            "400 / 1059\n",
            "401 / 1059\n",
            "402 / 1059\n",
            "403 / 1059\n",
            "404 / 1059\n",
            "405 / 1059\n",
            "406 / 1059\n",
            "407 / 1059\n",
            "408 / 1059\n",
            "409 / 1059\n",
            "410 / 1059\n",
            "411 / 1059\n",
            "412 / 1059\n",
            "413 / 1059\n",
            "414 / 1059\n",
            "415 / 1059\n",
            "416 / 1059\n",
            "417 / 1059\n",
            "418 / 1059\n",
            "419 / 1059\n",
            "420 / 1059\n",
            "421 / 1059\n",
            "422 / 1059\n",
            "423 / 1059\n",
            "424 / 1059\n",
            "425 / 1059\n",
            "426 / 1059\n",
            "427 / 1059\n",
            "428 / 1059\n",
            "429 / 1059\n",
            "430 / 1059\n",
            "431 / 1059\n",
            "432 / 1059\n",
            "433 / 1059\n",
            "434 / 1059\n",
            "435 / 1059\n",
            "436 / 1059\n",
            "437 / 1059\n",
            "438 / 1059\n",
            "439 / 1059\n",
            "440 / 1059\n",
            "441 / 1059\n",
            "442 / 1059\n",
            "443 / 1059\n",
            "444 / 1059\n",
            "445 / 1059\n",
            "446 / 1059\n",
            "447 / 1059\n",
            "448 / 1059\n",
            "449 / 1059\n",
            "450 / 1059\n",
            "451 / 1059\n",
            "452 / 1059\n",
            "453 / 1059\n",
            "454 / 1059\n",
            "455 / 1059\n",
            "456 / 1059\n",
            "457 / 1059\n",
            "458 / 1059\n",
            "459 / 1059\n",
            "460 / 1059\n",
            "461 / 1059\n",
            "462 / 1059\n",
            "463 / 1059\n",
            "464 / 1059\n",
            "465 / 1059\n",
            "466 / 1059\n",
            "467 / 1059\n",
            "468 / 1059\n",
            "469 / 1059\n",
            "470 / 1059\n",
            "471 / 1059\n",
            "472 / 1059\n",
            "473 / 1059\n",
            "474 / 1059\n",
            "475 / 1059\n",
            "476 / 1059\n",
            "477 / 1059\n",
            "478 / 1059\n",
            "479 / 1059\n",
            "480 / 1059\n",
            "481 / 1059\n",
            "482 / 1059\n",
            "483 / 1059\n",
            "484 / 1059\n",
            "485 / 1059\n",
            "486 / 1059\n",
            "487 / 1059\n",
            "488 / 1059\n",
            "489 / 1059\n",
            "490 / 1059\n",
            "491 / 1059\n",
            "492 / 1059\n",
            "493 / 1059\n",
            "494 / 1059\n",
            "495 / 1059\n",
            "496 / 1059\n",
            "497 / 1059\n",
            "498 / 1059\n",
            "499 / 1059\n",
            "500 / 1059\n",
            "501 / 1059\n",
            "502 / 1059\n",
            "503 / 1059\n",
            "504 / 1059\n",
            "505 / 1059\n",
            "506 / 1059\n",
            "507 / 1059\n",
            "508 / 1059\n",
            "509 / 1059\n",
            "510 / 1059\n",
            "511 / 1059\n",
            "512 / 1059\n",
            "513 / 1059\n",
            "514 / 1059\n",
            "515 / 1059\n",
            "516 / 1059\n",
            "517 / 1059\n",
            "518 / 1059\n",
            "519 / 1059\n",
            "520 / 1059\n",
            "521 / 1059\n",
            "522 / 1059\n",
            "523 / 1059\n",
            "524 / 1059\n",
            "525 / 1059\n",
            "526 / 1059\n",
            "527 / 1059\n",
            "528 / 1059\n",
            "529 / 1059\n",
            "530 / 1059\n",
            "531 / 1059\n",
            "532 / 1059\n",
            "533 / 1059\n",
            "534 / 1059\n",
            "535 / 1059\n",
            "536 / 1059\n",
            "537 / 1059\n",
            "538 / 1059\n",
            "539 / 1059\n",
            "540 / 1059\n",
            "541 / 1059\n",
            "542 / 1059\n",
            "543 / 1059\n",
            "544 / 1059\n",
            "545 / 1059\n",
            "546 / 1059\n",
            "547 / 1059\n",
            "548 / 1059\n",
            "549 / 1059\n",
            "550 / 1059\n",
            "551 / 1059\n",
            "552 / 1059\n",
            "553 / 1059\n",
            "554 / 1059\n",
            "555 / 1059\n",
            "556 / 1059\n",
            "557 / 1059\n",
            "558 / 1059\n",
            "559 / 1059\n",
            "560 / 1059\n",
            "561 / 1059\n",
            "562 / 1059\n",
            "563 / 1059\n",
            "564 / 1059\n",
            "565 / 1059\n",
            "566 / 1059\n",
            "567 / 1059\n",
            "568 / 1059\n",
            "569 / 1059\n",
            "570 / 1059\n",
            "571 / 1059\n",
            "572 / 1059\n",
            "573 / 1059\n",
            "574 / 1059\n",
            "575 / 1059\n",
            "576 / 1059\n",
            "577 / 1059\n",
            "578 / 1059\n",
            "579 / 1059\n",
            "580 / 1059\n",
            "581 / 1059\n",
            "582 / 1059\n",
            "583 / 1059\n",
            "584 / 1059\n",
            "585 / 1059\n",
            "586 / 1059\n",
            "587 / 1059\n",
            "588 / 1059\n",
            "589 / 1059\n",
            "590 / 1059\n",
            "591 / 1059\n",
            "592 / 1059\n",
            "593 / 1059\n",
            "594 / 1059\n",
            "595 / 1059\n",
            "596 / 1059\n",
            "597 / 1059\n",
            "598 / 1059\n",
            "599 / 1059\n",
            "600 / 1059\n",
            "601 / 1059\n",
            "602 / 1059\n",
            "603 / 1059\n",
            "604 / 1059\n",
            "605 / 1059\n",
            "606 / 1059\n",
            "607 / 1059\n",
            "608 / 1059\n",
            "609 / 1059\n",
            "610 / 1059\n",
            "611 / 1059\n",
            "612 / 1059\n",
            "613 / 1059\n",
            "614 / 1059\n",
            "615 / 1059\n",
            "616 / 1059\n",
            "617 / 1059\n",
            "618 / 1059\n",
            "619 / 1059\n",
            "620 / 1059\n",
            "621 / 1059\n",
            "622 / 1059\n",
            "623 / 1059\n",
            "624 / 1059\n",
            "625 / 1059\n",
            "626 / 1059\n",
            "627 / 1059\n",
            "628 / 1059\n",
            "629 / 1059\n",
            "630 / 1059\n",
            "631 / 1059\n",
            "632 / 1059\n",
            "633 / 1059\n",
            "634 / 1059\n",
            "635 / 1059\n",
            "636 / 1059\n",
            "637 / 1059\n",
            "638 / 1059\n",
            "639 / 1059\n",
            "640 / 1059\n",
            "641 / 1059\n",
            "642 / 1059\n",
            "643 / 1059\n",
            "644 / 1059\n",
            "645 / 1059\n",
            "646 / 1059\n",
            "647 / 1059\n",
            "648 / 1059\n",
            "649 / 1059\n",
            "650 / 1059\n",
            "651 / 1059\n",
            "652 / 1059\n",
            "653 / 1059\n",
            "654 / 1059\n",
            "655 / 1059\n",
            "656 / 1059\n",
            "657 / 1059\n",
            "658 / 1059\n",
            "659 / 1059\n",
            "660 / 1059\n",
            "661 / 1059\n",
            "662 / 1059\n",
            "663 / 1059\n",
            "664 / 1059\n",
            "665 / 1059\n",
            "666 / 1059\n",
            "667 / 1059\n",
            "668 / 1059\n",
            "669 / 1059\n",
            "670 / 1059\n",
            "671 / 1059\n",
            "672 / 1059\n",
            "673 / 1059\n",
            "674 / 1059\n",
            "675 / 1059\n",
            "676 / 1059\n",
            "677 / 1059\n",
            "678 / 1059\n",
            "679 / 1059\n",
            "680 / 1059\n",
            "681 / 1059\n",
            "682 / 1059\n",
            "683 / 1059\n",
            "684 / 1059\n",
            "685 / 1059\n",
            "686 / 1059\n",
            "687 / 1059\n",
            "688 / 1059\n",
            "689 / 1059\n",
            "690 / 1059\n",
            "691 / 1059\n",
            "692 / 1059\n",
            "693 / 1059\n",
            "694 / 1059\n",
            "695 / 1059\n",
            "696 / 1059\n",
            "697 / 1059\n",
            "698 / 1059\n",
            "699 / 1059\n",
            "700 / 1059\n",
            "701 / 1059\n",
            "702 / 1059\n",
            "703 / 1059\n",
            "704 / 1059\n",
            "705 / 1059\n",
            "706 / 1059\n",
            "707 / 1059\n",
            "708 / 1059\n",
            "709 / 1059\n",
            "710 / 1059\n",
            "711 / 1059\n",
            "712 / 1059\n",
            "713 / 1059\n",
            "714 / 1059\n",
            "715 / 1059\n",
            "716 / 1059\n",
            "717 / 1059\n",
            "718 / 1059\n",
            "719 / 1059\n",
            "720 / 1059\n",
            "721 / 1059\n",
            "722 / 1059\n",
            "723 / 1059\n",
            "724 / 1059\n",
            "725 / 1059\n",
            "726 / 1059\n",
            "727 / 1059\n",
            "728 / 1059\n",
            "729 / 1059\n",
            "730 / 1059\n",
            "731 / 1059\n",
            "732 / 1059\n",
            "733 / 1059\n",
            "734 / 1059\n",
            "735 / 1059\n",
            "736 / 1059\n",
            "737 / 1059\n",
            "738 / 1059\n",
            "739 / 1059\n",
            "740 / 1059\n",
            "741 / 1059\n",
            "742 / 1059\n",
            "743 / 1059\n",
            "744 / 1059\n",
            "745 / 1059\n",
            "746 / 1059\n",
            "747 / 1059\n",
            "748 / 1059\n",
            "749 / 1059\n",
            "750 / 1059\n",
            "751 / 1059\n",
            "752 / 1059\n",
            "753 / 1059\n",
            "754 / 1059\n",
            "755 / 1059\n",
            "756 / 1059\n",
            "757 / 1059\n",
            "758 / 1059\n",
            "759 / 1059\n",
            "760 / 1059\n",
            "761 / 1059\n",
            "762 / 1059\n",
            "763 / 1059\n",
            "764 / 1059\n",
            "765 / 1059\n",
            "766 / 1059\n",
            "767 / 1059\n",
            "768 / 1059\n",
            "769 / 1059\n",
            "770 / 1059\n",
            "771 / 1059\n",
            "772 / 1059\n",
            "773 / 1059\n",
            "774 / 1059\n",
            "775 / 1059\n",
            "776 / 1059\n",
            "777 / 1059\n",
            "778 / 1059\n",
            "779 / 1059\n",
            "780 / 1059\n",
            "781 / 1059\n",
            "782 / 1059\n",
            "783 / 1059\n",
            "784 / 1059\n",
            "785 / 1059\n",
            "786 / 1059\n",
            "787 / 1059\n",
            "788 / 1059\n",
            "789 / 1059\n",
            "790 / 1059\n",
            "791 / 1059\n",
            "792 / 1059\n",
            "793 / 1059\n",
            "794 / 1059\n",
            "795 / 1059\n",
            "796 / 1059\n",
            "797 / 1059\n",
            "798 / 1059\n",
            "799 / 1059\n",
            "800 / 1059\n",
            "801 / 1059\n",
            "802 / 1059\n",
            "803 / 1059\n",
            "804 / 1059\n",
            "805 / 1059\n",
            "806 / 1059\n",
            "807 / 1059\n",
            "808 / 1059\n",
            "809 / 1059\n",
            "810 / 1059\n",
            "811 / 1059\n",
            "812 / 1059\n",
            "813 / 1059\n",
            "814 / 1059\n",
            "815 / 1059\n",
            "816 / 1059\n",
            "817 / 1059\n",
            "818 / 1059\n",
            "819 / 1059\n",
            "820 / 1059\n",
            "821 / 1059\n",
            "822 / 1059\n",
            "823 / 1059\n",
            "824 / 1059\n",
            "825 / 1059\n",
            "826 / 1059\n",
            "827 / 1059\n",
            "828 / 1059\n",
            "829 / 1059\n",
            "830 / 1059\n",
            "831 / 1059\n",
            "832 / 1059\n",
            "833 / 1059\n",
            "834 / 1059\n",
            "835 / 1059\n",
            "836 / 1059\n",
            "837 / 1059\n",
            "838 / 1059\n",
            "839 / 1059\n",
            "840 / 1059\n",
            "841 / 1059\n",
            "842 / 1059\n",
            "843 / 1059\n",
            "844 / 1059\n",
            "845 / 1059\n",
            "846 / 1059\n",
            "847 / 1059\n",
            "848 / 1059\n",
            "849 / 1059\n",
            "850 / 1059\n",
            "851 / 1059\n",
            "852 / 1059\n",
            "853 / 1059\n",
            "854 / 1059\n",
            "855 / 1059\n",
            "856 / 1059\n",
            "857 / 1059\n",
            "858 / 1059\n",
            "859 / 1059\n",
            "860 / 1059\n",
            "861 / 1059\n",
            "862 / 1059\n",
            "863 / 1059\n",
            "864 / 1059\n",
            "865 / 1059\n",
            "866 / 1059\n",
            "867 / 1059\n",
            "868 / 1059\n",
            "869 / 1059\n",
            "870 / 1059\n",
            "871 / 1059\n",
            "872 / 1059\n",
            "873 / 1059\n",
            "874 / 1059\n",
            "875 / 1059\n",
            "876 / 1059\n",
            "877 / 1059\n",
            "878 / 1059\n",
            "879 / 1059\n",
            "880 / 1059\n",
            "881 / 1059\n",
            "882 / 1059\n",
            "883 / 1059\n",
            "884 / 1059\n",
            "885 / 1059\n",
            "886 / 1059\n",
            "887 / 1059\n",
            "888 / 1059\n",
            "889 / 1059\n",
            "890 / 1059\n",
            "891 / 1059\n",
            "892 / 1059\n",
            "893 / 1059\n",
            "894 / 1059\n",
            "895 / 1059\n",
            "896 / 1059\n",
            "897 / 1059\n",
            "898 / 1059\n",
            "899 / 1059\n",
            "900 / 1059\n",
            "901 / 1059\n",
            "902 / 1059\n",
            "903 / 1059\n",
            "904 / 1059\n",
            "905 / 1059\n",
            "906 / 1059\n",
            "907 / 1059\n",
            "908 / 1059\n",
            "909 / 1059\n",
            "910 / 1059\n",
            "911 / 1059\n",
            "912 / 1059\n",
            "913 / 1059\n",
            "914 / 1059\n",
            "915 / 1059\n",
            "916 / 1059\n",
            "917 / 1059\n",
            "918 / 1059\n",
            "919 / 1059\n",
            "920 / 1059\n",
            "921 / 1059\n",
            "922 / 1059\n",
            "923 / 1059\n",
            "924 / 1059\n",
            "925 / 1059\n",
            "926 / 1059\n",
            "927 / 1059\n",
            "928 / 1059\n",
            "929 / 1059\n",
            "930 / 1059\n",
            "931 / 1059\n",
            "932 / 1059\n",
            "933 / 1059\n",
            "934 / 1059\n",
            "935 / 1059\n",
            "936 / 1059\n",
            "937 / 1059\n",
            "938 / 1059\n",
            "939 / 1059\n",
            "940 / 1059\n",
            "941 / 1059\n",
            "942 / 1059\n",
            "943 / 1059\n",
            "944 / 1059\n",
            "945 / 1059\n",
            "946 / 1059\n",
            "947 / 1059\n",
            "948 / 1059\n",
            "949 / 1059\n",
            "950 / 1059\n",
            "951 / 1059\n",
            "952 / 1059\n",
            "953 / 1059\n",
            "954 / 1059\n",
            "955 / 1059\n",
            "956 / 1059\n",
            "957 / 1059\n",
            "958 / 1059\n",
            "959 / 1059\n",
            "960 / 1059\n",
            "961 / 1059\n",
            "962 / 1059\n",
            "963 / 1059\n",
            "964 / 1059\n",
            "965 / 1059\n",
            "966 / 1059\n",
            "967 / 1059\n",
            "968 / 1059\n",
            "969 / 1059\n",
            "970 / 1059\n",
            "971 / 1059\n",
            "972 / 1059\n",
            "973 / 1059\n",
            "974 / 1059\n",
            "975 / 1059\n",
            "976 / 1059\n",
            "977 / 1059\n",
            "978 / 1059\n",
            "979 / 1059\n",
            "980 / 1059\n",
            "981 / 1059\n",
            "982 / 1059\n",
            "983 / 1059\n",
            "984 / 1059\n",
            "985 / 1059\n",
            "986 / 1059\n",
            "987 / 1059\n",
            "988 / 1059\n",
            "989 / 1059\n",
            "990 / 1059\n",
            "991 / 1059\n",
            "992 / 1059\n",
            "993 / 1059\n",
            "994 / 1059\n",
            "995 / 1059\n",
            "996 / 1059\n",
            "997 / 1059\n",
            "998 / 1059\n",
            "999 / 1059\n",
            "1000 / 1059\n",
            "1001 / 1059\n",
            "1002 / 1059\n",
            "1003 / 1059\n",
            "1004 / 1059\n",
            "1005 / 1059\n",
            "1006 / 1059\n",
            "1007 / 1059\n",
            "1008 / 1059\n",
            "1009 / 1059\n",
            "1010 / 1059\n",
            "1011 / 1059\n",
            "1012 / 1059\n",
            "1013 / 1059\n",
            "1014 / 1059\n",
            "1015 / 1059\n",
            "1016 / 1059\n",
            "1017 / 1059\n",
            "1018 / 1059\n",
            "1019 / 1059\n",
            "1020 / 1059\n",
            "1021 / 1059\n",
            "1022 / 1059\n",
            "1023 / 1059\n",
            "1024 / 1059\n",
            "1025 / 1059\n",
            "1026 / 1059\n",
            "1027 / 1059\n",
            "1028 / 1059\n",
            "1029 / 1059\n",
            "1030 / 1059\n",
            "1031 / 1059\n",
            "1032 / 1059\n",
            "1033 / 1059\n",
            "1034 / 1059\n",
            "1035 / 1059\n",
            "1036 / 1059\n",
            "1037 / 1059\n",
            "1038 / 1059\n",
            "1039 / 1059\n",
            "1040 / 1059\n",
            "1041 / 1059\n",
            "1042 / 1059\n",
            "1043 / 1059\n",
            "1044 / 1059\n",
            "1045 / 1059\n",
            "1046 / 1059\n",
            "1047 / 1059\n",
            "1048 / 1059\n",
            "1049 / 1059\n",
            "1050 / 1059\n",
            "1051 / 1059\n",
            "1052 / 1059\n",
            "1053 / 1059\n",
            "1054 / 1059\n",
            "1055 / 1059\n",
            "1056 / 1059\n",
            "1057 / 1059\n",
            "1058 / 1059\n",
            "1059 / 1059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApA2FxElntA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('final_states.npy', final_states, allow_pickle = False)"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq-_AiXwxxFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_states = np.load('final_states.npy')\n",
        "\n",
        "_last_states = dataset.df.iloc[dataset.offsets[:-1] - 1].sort_values(by = ['session']).item.values\n",
        "\n",
        "g4r.model.reset_states()\n",
        "\n",
        "y = np.empty(shape = (dataset.n_sessions, g4r.n_classes))\n",
        "y[:] = None\n",
        "X = np.empty(shape = (g4r.batch_size, 1, g4r.n_classes))\n",
        "for batch_id in range(len(_last_states) // g4r.batch_size):\n",
        "    X[:] = None\n",
        "    for i in range(g4r.batch_size):\n",
        "        X[i, :] = dataset.item_to_one_hot[_last_states[batch_id * g4r.batch_size + i]]\n",
        "    nlg = 0\n",
        "    for nl in range(len(g4r.model.layers)):\n",
        "        if g4r.model.layers[nl].name.startswith('GRU_'):\n",
        "            g4r.model.layers[nl].reset_states(final_states[batch_id * g4r.batch_size : (batch_id + 1) * g4r.batch_size, nlg, :])\n",
        "            nlg += 1\n",
        "    y[batch_id * g4r.batch_size : (batch_id + 1) * g4r.batch_size, :] = g4r.model.predict(X)[:g4r.batch_size]\n",
        "\n",
        "y = tf.constant(y[:32 * 33], dtype = tf.float32)"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VbvtLjYukHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = np.empty(shape = (dataset.n_sessions, dataset.n_items))\n",
        "for i in range(y_true.shape[0]):\n",
        "    y_true[i, :] = dataset.item_to_one_hot[dfTest.item.values[i]]\n",
        "\n",
        "y_true = tf.constant(y_true[:32*33], dtype = tf.float32)"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtOnfupXvDrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f827e92-9de3-4dea-e78a-8bca8b7e6ce7"
      },
      "source": [
        "tf.reduce_sum(tf.keras.metrics.top_k_categorical_accuracy(y_true, y, k = 3)) / y_true.shape[0]"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.9640151>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 263
        }
      ]
    }
  ]
}